{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controllability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import shutil\n",
    "import numpy as np\n",
    "import imageio\n",
    "from itertools import chain\n",
    "import torch\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0, \"../../SemanticStyleGAN\")\n",
    "from models import make_model\n",
    "from visualize.utils import generate, cubic_spline_interpolate\n",
    "from utils.control import Control\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "from numpy import cov \n",
    "from numpy.linalg import eig\n",
    "from torch import nn\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sefa(model,local_nets,layers,take_conv_weights=True):\n",
    "    '''\n",
    "    Function to get certain weights out of the SSG Model.\n",
    "    local_nets between 0 and 15\n",
    "    layers between 0 and 9\n",
    "    '''\n",
    "    all_local_nets=model.__getattr__(\"local_nets\")\n",
    "    weights = []\n",
    "    if not type(layers)==list:\n",
    "        layers=[layers]\n",
    "    for l_net in local_nets:\n",
    "        for layer in layers:\n",
    "            weight_temp_conv=all_local_nets[l_net].__getattr__(\"linears\")[layer].__getattr__(\"conv\").weight.squeeze(0)\n",
    "            weight_temp_modulation=all_local_nets[l_net].__getattr__(\"linears\")[layer].__getattr__(\"conv\").__getattr__(\"modulation\").weight\n",
    "            weight_temp_conv = weight_temp_conv.flip(2, 3).permute(1, 0, 2, 3).flatten(1)\n",
    "            if take_conv_weights:\n",
    "                 weights.append(weight_temp_conv.cpu().detach().numpy())\n",
    "            else:\n",
    "                 weights.append(weight_temp_modulation.cpu().detach().numpy())\n",
    "    weight = np.concatenate(weights, axis=1).astype(np.float32)\n",
    "    weight = weight / np.linalg.norm(weight, axis=0, keepdims=True)\n",
    "    eigen_values, eigen_vectors = np.linalg.eig(weight.dot(weight.T))\n",
    "\n",
    "    return eigen_vectors.T,eigen_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intialize control and load latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt=\"/no_backups/g013/checkpoints/SSG_v3.13/ckpt/140000.pt\"\n",
    "device=\"cpu\"\n",
    "control = Control(ckpt,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Styles for Cityscape\n",
    "latent1= \"../results/saved_samples/first_latent.npy\"\n",
    "latent2= \"../results/saved_samples/second_latent.npy\"\n",
    "latent3= \"../results/saved_samples/third_latent.npy\"\n",
    "latent4=\"../results/saved_samples/fourth_latent.npy\"\n",
    "latent5=\"../results/saved_samples/fifth_latent.npy\"\n",
    "latent6=\"../results/saved_samples/sixth_latent.npy\"\n",
    "latent7=\"../results/saved_samples/seventh_latent.npy\"\n",
    "latent_mean=\"../results/saved_samples/mean_latent.npy\"\n",
    "table_2_input = \"../results/saved_samples/table_2_input.npy\"\n",
    "styles1 = torch.tensor(np.load(latent1), device=device)\n",
    "styles2 = torch.tensor(np.load(latent2), device=device)\n",
    "styles3 = torch.tensor(np.load(latent3), device=device)\n",
    "styles4 = torch.tensor(np.load(latent4), device=device)\n",
    "styles5 = torch.tensor(np.load(latent5), device=device)\n",
    "styles6 = torch.tensor(np.load(latent6), device=device)\n",
    "styles7 = torch.tensor(np.load(latent7), device=device)\n",
    "styles_mean = torch.tensor(np.load(latent_mean), device=device)\n",
    "styles_t2= torch.tensor(np.load(table_2_input), device=device)\n",
    "styles1 = styles1.unsqueeze(1).repeat(1, control.model.n_latent, 1)\n",
    "styles2 = styles2.unsqueeze(1).repeat(1, control.model.n_latent, 1)\n",
    "styles3 = styles3.unsqueeze(1).repeat(1, control.model.n_latent, 1)\n",
    "styles4 = styles4.unsqueeze(1).repeat(1, control.model.n_latent, 1)\n",
    "styles5 = styles5.unsqueeze(1).repeat(1, control.model.n_latent, 1)\n",
    "styles6 = styles6.unsqueeze(1).repeat(1, control.model.n_latent, 1)\n",
    "styles7 = styles7.unsqueeze(1).repeat(1, control.model.n_latent, 1)\n",
    "styles_mean = styles_mean.unsqueeze(1).repeat(1, control.model.n_latent, 1)\n",
    "styles_t2 = styles_t2.unsqueeze(1).repeat(1, control.model.n_latent, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###Styles for Mapillary\n",
    "# latent1= \"../results/mapillary_picked_samples/000000_latent.npy\"\n",
    "# latent2= \"../results/mapillary_picked_samples/000003_latent.npy\"\n",
    "# latent3= \"../results/mapillary_picked_samples/000004_latent.npy\"\n",
    "# latent4=\"../results/mapillary_picked_samples/000005_latent.npy\"\n",
    "# latent5=\"../results/mapillary_picked_samples/000007_latent.npy\"\n",
    "# latent6=\"../results/mapillary_picked_samples/000009_latent.npy\"\n",
    "# latent7=\"../results/mapillary_picked_samples/000017_latent.npy\"\n",
    "# latent_mean=\"../results/mapillary_picked_samples/mean_latent.npy\"\n",
    "\n",
    "\n",
    "# styles1 = torch.tensor(np.load(latent1), device=device)\n",
    "# styles2 = torch.tensor(np.load(latent2), device=device)\n",
    "# styles3 = torch.tensor(np.load(latent3), device=device)\n",
    "# styles4 = torch.tensor(np.load(latent4), device=device)\n",
    "# styles5 = torch.tensor(np.load(latent5), device=device)\n",
    "# styles6 = torch.tensor(np.load(latent6), device=device)\n",
    "# styles7 = torch.tensor(np.load(latent7), device=device)\n",
    "# styles_mean = torch.tensor(np.load(latent_mean), device=device)\n",
    "\n",
    "# styles1 = styles1.unsqueeze(1).repeat(1, control.model.n_latent, 1)\n",
    "# styles2 = styles2.unsqueeze(1).repeat(1, control.model.n_latent, 1)\n",
    "# styles3 = styles3.unsqueeze(1).repeat(1, control.model.n_latent, 1)\n",
    "# styles4 = styles4.unsqueeze(1).repeat(1, control.model.n_latent, 1)\n",
    "# styles5 = styles5.unsqueeze(1).repeat(1, control.model.n_latent, 1)\n",
    "# styles6 = styles6.unsqueeze(1).repeat(1, control.model.n_latent, 1)\n",
    "# styles7 = styles7.unsqueeze(1).repeat(1, control.model.n_latent, 1)\n",
    "# styles_mean = styles_mean.unsqueeze(1).repeat(1, control.model.n_latent, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get and Save EigenValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_index=13\n",
    "tree_index=9\n",
    "road_index=1\n",
    "## From 0->9 all layers\n",
    "eigen_vecs,eigen_vals=calculate_sefa(control.model,[road_index],np.arange(10))\n",
    "##From 2->6 shape \n",
    "eigen_vecs_shape,eigen_vals_shape=calculate_sefa(control.model,[road_index],np.arange(2,6))\n",
    "##From 6->10 Texture \n",
    "eigen_vecs_texture,eigen_vals_texture=calculate_sefa(control.model,[road_index],np.arange(6,10))\n",
    "\n",
    "## Expanding the tensor from 64x64 to 64x512\n",
    "# eigen_vecs = torch.repeat_interleave(torch.tensor(eigen_vecs),8,dim=1).numpy()\n",
    "# eigen_vecs_shape = torch.repeat_interleave(torch.tensor(eigen_vecs_shape),8,dim=1).numpy()\n",
    "# eigen_vecs_texture = torch.repeat_interleave(torch.tensor(eigen_vecs_texture),8,dim=1).numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Editing Images using GAN-Space in W Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Effects of continous changing of 1 component\n",
    "\n",
    "apply_to_all_layers=False\n",
    "class_index=9\n",
    "latent_index = 28\n",
    "latent_name=\"car_shape\"\n",
    "component=9\n",
    "coarse_layer=1\n",
    "# chain_1 = range(0,0,1) # Set aside for now \n",
    "# chain_2 =  np.arange(10,-10,-0.1)\n",
    "# mult_range = chain(chain_1,chain_2)\n",
    "#V=eigen_vecs_shape\n",
    "V=torch.load(\"../results/mapillary_picked_samples/principal_components_2_mapillary.pt\")\n",
    "\"trees/cars/road/building\"\n",
    "classes= [\"vegi_texture\"]\n",
    "latent_indices=[21,29,5,9]\n",
    "for class_name,latent_index in zip (classes,latent_indices):\n",
    "    for component in [4]:\n",
    "        print(f\"Processing component {component}\")\n",
    "        #[styles_mean,styles1,styles3]\n",
    "        for i,style_chosen in enumerate([styles1,styles2,styles3,styles4,styles5,styles6,styles7,styles_mean]):\n",
    "            print(f\"Styles{i+1}\")    \n",
    "            images=[]\n",
    "            segs=[]\n",
    "            for multiplier in  np.linspace(60,-60,40):\n",
    "                #print(f\"Analyzing COMPONENT {component} with multiplier {multiplier}\")\n",
    "                #image,seg=control.edit_image_inject_coarse(class_index,multiplier,style_chosen,V[component],coarse_inject_layer=coarse_layer,plot=False,get_image=True)\n",
    "                #latent_index = np.arange(5, 34, 2) ##For Editing all textures at once.\n",
    "                image,seg=control.edit_image_principal_component(latent_index,class_index,multiplier,style_chosen,V[component],whole_image=apply_to_all_layers,plot=False,get_image=True)\n",
    "                images.append(image[0])\n",
    "                segs.append(seg[0])\n",
    "            # images = images + images[::-1]\n",
    "            # segs= segs + segs[::-1]\n",
    "            control.images_to_video(images,segs,f\"./data/thesis_results_mapillary/W_space_class_name_{class_name}{component}_for_image_{i+1}.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of a mean Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creation of a mean image to easily see the effect of one S space direction.\n",
    "styles = control.model.style(\n",
    "        torch.randn(50000, control.model.style_dim, device=\"cpu\")\n",
    "    )\n",
    "\n",
    "mean_style=styles.mean(0).unsqueeze(0)\n",
    "np.save(\"../results/mapillary_picked_samples/mean_latent.npy\",mean_style.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of applying S space directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_w_to_w_extended(model,w_vectors,local_generator,layer):\n",
    "    '''\n",
    "    A function that converts given w_vectors of size Nx512 to Nx64 given a specific demodulation in the model.\n",
    "    '''\n",
    "    modulation=control.model.__getattr__(\"local_nets\")[local_generator].__getattr__(\"linears\")[layer].__getattr__(\"conv\").modulation\n",
    "    return modulation(w_vectors)\n",
    "    \n",
    "def prepare_w_extended(model,style,w_extended,class_index,layers):\n",
    "    assert len(style.shape)==1\n",
    "    for layer_index in layers:\n",
    "         w_extended[class_index][layer_index]=convert_from_w_to_w_extended(model,style,class_index,layer_index).detach()\n",
    "    return w_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pca_2(samples,selected=10):\n",
    "    samples_cop = samples.cpu().detach().numpy()\n",
    "    M = mean(samples_cop)\n",
    "    C = samples_cop-M\n",
    "    V_2=cov(C.T)\n",
    "    values, vectors = eig(V_2)\n",
    "    return values[:selected],vectors[:selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pca_from_s_space(model,local_generator,layer):\n",
    "    styles =model.style(\n",
    "        torch.randn(50000, model.style_dim, device=\"cpu\")\n",
    "    )\n",
    "    styles_converted=convert_from_w_to_w_extended(control.model,styles,local_generator,layer)\n",
    "    _,vectors = calculate_pca_2(styles_converted,selected=513)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv=calculate_pca_from_s_space(control.model,13,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Extraction of PCA directions from the converted w extended space\n",
    "truncation_mean = 10000\n",
    "truncation=0.7\n",
    "mean_latent =control.model.style(\n",
    "    torch.randn(truncation_mean, control.model.style_dim, device=\"cpu\")\n",
    ").mean(0)\n",
    "\n",
    "styles = control.model.style(\n",
    "        torch.randn(50000, control.model.style_dim, device=\"cpu\")\n",
    "    )\n",
    "##Truncation 50k styles.\n",
    "#styles = truncation * styles + (1 - truncation) * mean_latent.unsqueeze(0)\n",
    "local_generator=13\n",
    "layer=5\n",
    "styles_converted=convert_from_w_to_w_extended(control.model,styles,local_generator,layer)\n",
    "res_2 = calculate_pca_2(styles_converted,selected=513)\n",
    "vectors = res_2[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Effects of continous changing of 1 component\n",
    "\n",
    "apply_to_all_layers=False\n",
    "latent_index=4\n",
    "class_index=13\n",
    "layer_index=5\n",
    "layer_index=np.arange(2,6)\n",
    "latent_name=\"car_shape\"\n",
    "w_extended=torch.zeros(16,10,64)\n",
    "\n",
    "V,_=calculate_sefa(control.model,[class_index],layer_index)\n",
    "for component in [6,7,8,9,10]:\n",
    "    print(f\"Processing component {component}\")\n",
    "    for i,style_chosen in enumerate([styles_mean,styles1,styles2,styles3,styles5]):\n",
    "        print(f\"Styles{i+1}\")    \n",
    "        images=[]\n",
    "        segs=[]\n",
    "        for multiplier in  np.linspace(-30,30,35):\n",
    "            w_extended_copy = w_extended.clone().detach()\n",
    "            w_extended_copy=prepare_w_extended(control.model,style_chosen[0][0],w_extended_copy,class_index,layer_index)\n",
    "            w_extended_copy[class_index][layer_index]+=(multiplier*V[component])\n",
    "            #print(f\"Analyzing COMPONENT {component} with multiplier {multiplier}\")\n",
    "            image,seg= control.edit_image_inject_modulation(class_index,style_chosen,w_extended_copy,plot=False,get_image=True)\n",
    "            images.append(image[0])\n",
    "            segs.append(seg[0])\n",
    "\n",
    "        control.images_to_video(images,segs,f\"./data/64_v/shape_layers_sefa/for_image_{i+1}_component_{component}_class_{class_index}_layer_{layer_index}.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring Further why sefa is not producing good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training and testing cell for both sefa and PCa on S space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Effects of continous changing of 1 component\n",
    "apply_to_all_layers=False\n",
    "latent_index=4\n",
    "class_index = 1 #Road\n",
    "class_index = 13 #Car\n",
    "class_index = 9 #Vegi.\n",
    "class_index = 10 #Sky\n",
    "component=9\n",
    "coarse_layer=1\n",
    "w_extended=torch.zeros(16,10,64)\n",
    "shape_layers=[2,3,4,5]\n",
    "texture_layers=[6,7,8,9]\n",
    "#Training list of format :\n",
    "'''\n",
    "[ [Class_index,LocalGeneratorLayer, [V_components]] , ...another training param..]\n",
    "'''\n",
    "\n",
    "top_10_dir=[0,1,2,3,4,5,6,7,8,9]\n",
    "training_list = [[10,[5],top_10_dir],\n",
    "[10,[4],top_10_dir],\n",
    "[10,[3],top_10_dir],\n",
    "[10,[9],top_10_dir],\n",
    "[10,[8],top_10_dir],\n",
    "[10,[7],top_10_dir],]\n",
    "training_list=[[13,[5],[1,2]],\n",
    "                [13,[4],[1,2]]]\n",
    "#Training List After\n",
    "# training_list = [[13,[5],[9,10,11,12]],\n",
    "#                 [13,[4],[9,10,11,12]],\n",
    "#                 [13,[4,5],[0,1,2,3,4,5,6]],\n",
    "# ]            \n",
    "#training_list = [[13,[6,7,8,9],[0,1,2,3,4,5,6,7,8]]]\n",
    "\n",
    "#Mapillary related results.\n",
    "vegi_focused_styles_mp=[styles1,styles5,styles_mean]\n",
    "sky_focused_styles_mp=[styles3,styles5]\n",
    "car_focuses_styles_mp=[styles6,styles7]\n",
    "#CityScapes related res\n",
    "vegi_focused_styles_cs=[styles1,styles4,styles7]\n",
    "\n",
    "styles_list=[styles_mean]\n",
    "saved_pcas=[]\n",
    "#V=calculate_pca_from_s_space(control.model,class_index,layer_index)\n",
    "for training_instance in training_list:\n",
    "    class_index = training_instance[0]\n",
    "    layer_index= training_instance[1]\n",
    "    components = training_instance[2]\n",
    "    print(f\"Processing class_index : {class_index}  layer index: {layer_index} with components {components}\")\n",
    "    #Notice that currently we are calculating weights from modulated part of the layer instead of main conv layer.\n",
    "    \n",
    "    \n",
    "    #V,_=calculate_sefa(control.model,[class_index],layer_index,take_conv_weights=True)\n",
    "    #V=calculate_pca_from_s_space(control.model,class_index,layer_index[0])\n",
    "    #saved_pcas.append(V)\n",
    "\n",
    "    save_path=f\"./thesis_related_results/saved_numpy/layer_{layer_index[0]}_class_index_{class_index}.npy\"\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        np.save(save_path,V)\n",
    "    else:\n",
    "        print(f\"Found numpy file with the same exact directions, loading  ...\")\n",
    "        V = np.load(save_path)\n",
    "        #raise Exception(\"File already exists, you sure you want to overwrite ?\")\n",
    "    #V_2 = np.load(f\"./data/mapillary/s_space/layer_{4}_class_index_{class_index}.npy\")\n",
    "    for component in components:\n",
    "        print(f\"Processing component {component}\")\n",
    "        for i,style_chosen in enumerate(styles_list):\n",
    "            print(f\"Styles{i+1}\")    \n",
    "            images=[]\n",
    "            segs=[]\n",
    "            for multiplier in  np.linspace(-70,70,18):\n",
    "                w_extended_copy = w_extended.clone().detach()\n",
    "                w_extended_copy=prepare_w_extended(control.model,style_chosen[0][0],w_extended_copy,class_index,layer_index)\n",
    "                w_extended_copy[class_index][layer_index[0]]+=((multiplier)*V[component])\n",
    "                #w_extended_copy[class_index][4]+=((multiplier)*V_2[1])\n",
    "                ##For Double EFFECT !!!! Remove !!!\n",
    "                #print(f\"Analyzing COMPONENT {component} with multiplier {multiplier}\")\n",
    "                image,seg= control.edit_image_inject_modulation(class_index,style_chosen,w_extended_copy,plot=False,get_image=True)\n",
    "                images.append(image[0])\n",
    "                segs.append(seg[0])\n",
    "            control.images_to_video(images,segs,f\"./thesis_related_results/table_2/retake/s_gan_space_layer_{layer_index}_{component}_component_for_image_{i+1}__class_{class_index}_xxxx.mp4\")\n",
    "            #control.images_to_video(images,segs,f\"./data/mapillary/s_space/class_index={class_index}/layer_{layer_index}_component_{component}_for_image_{i+1}__class_{class_index}.mp4\")\n",
    "            #control.images_to_video(images,segs,f\"./thesis_related_results/s_space/class_index={class_index}/layer_{layer_index}_{component}_component_for_image_{i+1}__class_{class_index}.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d7fb738a222c2b75f597de638cb4a5050e01a8d2927f5b3984dbaad8d93e00a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
